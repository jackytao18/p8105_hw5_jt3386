---
title: "Solutions for Homework 5"
author: "Jiajun Tao"
date: "2022-11-05"
output: github_document
---

```{r, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1

Our goal is to create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

We first created a dataframe containing all file names using `list.files` function. Then we added the relative path to the file names in order to be used in `read_csv` function. We iterated over file names and read in data for each subject using `map` and saving the result as a new variable in the dataframe. After that we unnested the data and do some cleaning. We added variables including arm, subject ID, and made the week as a variable using `pivot_longer`.

```{r}
files_df = tibble(
  files_name = list.files("data/problem_1/")) %>% 
  mutate(
    files_path = str_c("data/problem_1/",files_name),
    data = map(files_path, read_csv)
  ) %>% 
  unnest(data) %>% 
  select(-files_path) %>% 
  mutate(
    files_name = str_remove(files_name,".csv")
  ) %>% 
  separate(files_name, into = c("arm", "subject_id"), sep = "_") %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observations",
    names_prefix = "week_"
  )

files_df

files_df %>% 
  ggplot(aes(x = week, y = observations, group = subject_id, color = arm)) +
  geom_point() +
  geom_path() +
  facet_grid(. ~ arm)
```

We made a spaghetti plot showing observations on each subject over time. As we can see in the plot, the observations are obviously higher in the experimental arm than in the control arm. What's more, in the control arm, the observations seem to be at the same level over time, but in the experimental arm, the observations increase as time goes by.


### Problem 2

First, we imported the data.

```{r}
homicides_df = read_csv("data/problem_2/homicide-data.csv") 

homicides_df
```

The raw data has `r nrow(homicides_df)` rows and `r ncol(homicides_df)` columns. The variables include `r names(homicides_df)`.

Then we created a `city_state` variable and summarized within cities to obtain the total number of homicides and the number of unsolved homicides. I found that one observation might have a typo. The city was Tulsa, but the state was AL. I thought it should be in OK, so I just corrected it.

```{r}
homicides_df = 
  homicides_df %>% 
  mutate(
    state = ifelse(city == "Tulsa",
                   "OK",
                   state),
    city_state = str_c(city, ", ", state)
  ) %>% 
  group_by(city_state) %>% 
  summarise(
    n_total = n(),
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homicides_df
```

We used `prop.test` to estimate the proportion of homicides that are unsolved in Baltimore, MD, and pulled the estimated proportion and confidence intervals.

```{r}
baltimore_df =
  homicides_df %>% 
  filter(city_state == "Baltimore, MD") 

output_p_test = 
  prop.test(x = baltimore_df$n_unsolved,
            n = baltimore_df$n_total)

output_p_test %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

Now we ran `prop.test` for each of the cities in my dataset, and extracted both the proportion of unsolved homicides and the confidence interval for each. 

```{r}
prop_test_df = 
  homicides_df %>% 
  mutate(
    output_p_test = map2(.x = n_unsolved,
                         .y = n_total, 
                         ~broom::tidy(prop.test(x = .x, n = .y)))
  ) %>% 
  unnest(output_p_test) %>% 
  rename(estimated_proportion = estimate) %>% 
  select(city_state, estimated_proportion, conf.low, conf.high)

prop_test_df
```

Finally we created a plot to show the estimates and CIs for each city in order.

```{r}
prop_test_df %>% 
  mutate(
    city_state = fct_reorder(city_state, estimated_proportion)
  ) %>% 
  ggplot(aes(x = city_state, y = estimated_proportion)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


### Problem 3

We write a function to do simulation.

```{r}
set.seed(1234)
sim_t_test = function(n = 30, mu, sigma = 5) {
    
  sample = rnorm(n = n, mean = mu, sd = sigma)
  
  test_results = t.test(sample)
  
  test_results %>% 
    broom::tidy()
}
```

Set μ=0. Generate 5000 datasets from the model.

```{r}
results_df = 
  expand.grid(
    mu = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    test_result = map(.x = mu, ~ sim_t_test(mu = .x))
  ) %>% 
  unnest(test_result) %>% 
  select(mu, estimate, p.value)

results_df
```

Repeat the above for μ={1,2,3,4,5,6}

```{r}
sim_df = 
  expand.grid(
    mu = 1:6,
    iter = 1:5000
  ) %>% 
  mutate(
    test_result = map(.x = mu, ~ sim_t_test(mu = .x))
  ) %>% 
  unnest(test_result) %>% 
  select(mu, estimate, p.value)

sim_df
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.

```{r}
sim_df %>% 
  group_by(mu) %>% 
  summarise(
    proportion = sum(p.value < 0.05) / n()
  ) %>% 
  ggplot(aes(x = mu, y = proportion)) + 
  geom_point() +
  geom_path() +
  labs(x = "True value of mu", y = "The power of the test")
```

The power of the test increases as the effect size increases, and the slope or the increasing rate is getting smaller as the effect size increases.

Make a plot showing the average estimate of μ_hat on the y axis and the true value of μ on the x axis. Make a second plot the average estimate of μ_hat only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.

```{r}
plot_1 = 
  sim_df %>% 
  group_by(mu) %>% 
  summarise(
    average = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = average)) + 
  geom_point() + 
  geom_path() +
  labs(x = "True value of mu", y = "Average estimate of mu hat")

plot_2 =
  sim_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarise(
    average = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = average)) + 
  geom_point() + 
  geom_path() +
  labs(x = "True value of mu", y = "Average estimate of mu hat when the null is rejected")

plot_1 + plot_2
```

When the effect size is small, the sample average of mu hat when the null is rejected is very different from the true value of mu, actually it's always larger than the true value of mu. However, when the effect size gets larger, the sample average of mu hat when the null is rejected is approximately equal to the true value of mu. That's because as the power is increasing.

So given the small sample size and low power, and given that the null is rejected, we can see the average is very different from the true mean. It indicates the publication bias. We should be careful of this situation. When the power is low, the sample size is small, the results that the null is rejected may be far away from the truth.